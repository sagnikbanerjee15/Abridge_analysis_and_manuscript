#! /project/maizegdb/sagnik/softwares/anaconda/bin/python

from argparse import RawTextHelpFormatter
import argparse 
import logging
import os
import pprint
import sys
import re
import time
import multiprocessing
import random
import glob
from _ast import Or

def parseCommandLineArguments():
    parser = argparse.ArgumentParser(prog="abridge_reproduce_entire_analysis",description="Program will regenerate all the files, process them and also construct the graphs for publication. Please note that some of the data represents duration of execution and will not be reproducible. But the trend should hold true",formatter_class=RawTextHelpFormatter)
    required_named = parser.add_argument_group('Required arguments')
    optional_named = parser.add_argument_group('Optional arguments')
    
    ##################################################################################################
    # Required arguments
    ##################################################################################################
    required_named.add_argument("-md","--metadata",help="Enter the metadata file. The file format should be same as ListOfNCBI-SRASamplesForExperimentST.csv",required=True)
    required_named.add_argument("-odir","--output_directory",help="Enter the name of the output directory. all analysis will be stored here. Please make sure you have sufficient space on your disk to reproduce all the analysis",required=True)
    required_named.add_argument("--genome","-g",help="Enter the location of the genome",required=True)
    
    ##################################################################################################
    # Optional arguments
    ##################################################################################################
    optional_named.add_argument("--cpu","-n",help="Enter the number of CPUs. Please note that all alignments will be conducted using a single CPU. This argument will control how many parallel alignments can be lanuched", default=1)
    optional_named.add_argument("--num_times","-t",help="Enter the number of times you wish each process to execute. This will be handy in calculating the average time required for compression, decompression, retrieval and other operations", default = 5)
    optional_named.add_argument("--error_directory","-edir",help="Enter a directory where all error files will be stored. If nothing is specified then error files will be stored in the output directory",default=None)
    
    ##################################################################################################
    # Suppressed arguments
    ##################################################################################################
    parser.add_argument("--metadata_expanded","-metadata_expanded",help=argparse.SUPPRESS)
    parser.add_argument("--sra_list_to_be_downloaded","-sra_list_to_be_downloaded",help=argparse.SUPPRESS)
    parser.add_argument("--logfilename","-logfilename",help=argparse.SUPPRESS)# Name of the logfile
     
    return parser.parse_args()

def readMetadataFile(options):
    """
    Reads in the metadata information and restructrues the data
    """
    data = {}
    fhr=open(options.metadata,"r")
    for line in fhr:
        if "Accession" in line:continue
        Organism,Tissue,Layout,Assay_Type,Date_of_publication, Length, Accession,Bioproject,Number_of_reads_or_pairs,Replicate_information, Notes  = line.strip().split(",")
        if Assay_Type not in data:
            data[Assay_Type] = []
        data[Assay_Type].append([Organism,Tissue,Layout,Assay_Type,Date_of_publication, Length, Accession,Bioproject, Number_of_reads_or_pairs,Replicate_information, Notes])
    fhr.close()
    options.metadata_expanded = data
    
def downloadData(options):
    """
    Downloads the data from NCBI-SRA
    """
    options.sra_list_to_be_downloaded = f"{options.output_directory}/sra_list_to_be_downloaded"
    fhw = open(options.sra_list_to_be_downloaded,"w")
    for Assay_Type in options.metadata_expanded:
        for row in options.metadata_expanded[Assay_Type]:
            fhw.write(row[6] + "\n")
    fhw.close()
    
    cmd  = f"downloadAndDumpFastqFromSRA.py "
    cmd += f" --cpu {options.cpu} "
    cmd += f" --sra {options.sra_list_to_be_downloaded} "
    cmd += f" --output {options.output_directory}/raw_data "
    cmd += f" 1> {options.output_directory}/outputs/raw_data_download.output "
    cmd += f" 2> {options.output_directory}/errors/raw_data_download.error "
    os.system(cmd)

def configureLogger(options):
    if os.path.exists(options.logfilename)==True:
        os.system(f"rm {options.logfilename}")
    logging.basicConfig(format='%(asctime)s - %(message)s', datefmt='%d-%b-%y %H:%M:%S',level=logging.DEBUG, filename=options.logfilename)

def runCommand(eachpinput):
    cmd,dummy = eachpinput
    os.system(cmd)

def mergeReadPairs(eachinput):
    options,sra=eachinput
    pair1_filename = f"{options.output_directory}/raw_data/{sra}_1.fastq"
    pair2_filename = f"{options.output_directory}/raw_data/{sra}_2.fastq"
    merged_filename = f"{options.output_directory}/raw_data/{sra}.fastq"
    if os.path.exists(merged_filename)==True:return
    fhw=open(merged_filename,"w")
    for fhr in [open(pair1_filename,"r") ,open(pair2_filename,"r")]:
        for line_num,line in enumerate(fhr):
            if line_num % 4 == 0 and line[0]=='@' and '/' in line:
                line = line.replace('/','_')
            fhw.write(line)
    fhw.close()

def main():
    commandLineArg=sys.argv
    if len(commandLineArg)==1:
        print("Please use the --help option to get usage information")
    options=parseCommandLineArguments()
    
    options.logfilename = options.output_directory+"/progress.log"
    configureLogger(options)
    
    pool = multiprocessing.Pool(processes=int(options.cpu))
    ##################################################################################################
    # Create output directory and subdirectories underneath
    ##################################################################################################
    create_these_directories = [options.output_directory,
                                f"{options.output_directory}/raw_data",
                                f"{options.output_directory}/star_alignments",
                                f"{options.output_directory}/star_index",
                                f"{options.output_directory}/abridge_compressed_files",
                                f"{options.output_directory}/abridge_decompressed_files",
                                f"{options.output_directory}/outputs",
                                f"{options.output_directory}/errors",
                                f"{options.output_directory}/final_output_files",
                                f"{options.output_directory}/final_output_files/graphs"]
    
    for d in create_these_directories:
        os.system(f"mkdir -p {d}")
    
    ##################################################################################################
    # Read in the metadata file
    ##################################################################################################
    readMetadataFile(options)
    logging.info(f"readMetadataFile() execution is complete")
    
    ##################################################################################################
    # Downloading the raw data from NCBI
    ##################################################################################################
    downloadData(options)
    logging.info(f"Data download from NCBI is complete")
    
    ##################################################################################################
    # Construct STAR index
    ##################################################################################################
    cmd  = "STAR "
    cmd += f" --runThreadN {options.cpu}"
    cmd += f" --genomeSAindexNbases 12 "
    cmd += f" --genomeDir {options.output_directory}/star_index "
    cmd += f" --genomeFastaFiles {options.genome}"
    cmd += f" --runMode genomeGenerate " 
    if os.path.exists(f"{options.output_directory}/star_index/genomeParameters.txt")==False:
        os.system(cmd)
        logging.info(f"STAR index generation is complete")
        
    ##################################################################################################
    # Merge read pairs to form one file
    ##################################################################################################
    list_of_commands = []
    for Assay_Type in options.metadata_expanded:
        for row in options.metadata_expanded[Assay_Type]:
            list_of_commands.append([options,row[6]])
    
    pool.map(mergeReadPairs,list_of_commands)
    logging.info(f"Merge complete")
            
    ##################################################################################################
    # Prepare metadata for alignemnt
    # A single .csv file will be prepared for both SE and PE samples
    # Organism,Tissue,Layout,Assay_Type,Date_of_publication,Read_Length,SRA 
    # Organism,Tissue,Layout,Assay_Type,Date_of_publication, Length, Accession,Bioproject, Number_of_reads_or_pairs,Replicate_information, Notes]
    ##################################################################################################
    fhw = open(f"{options.output_directory}/metadata_for_alignment.csv","w")
    add_these_back_to_metadata_expanded = {}
    for Assay_Type in options.metadata_expanded:
        add_these_back_to_metadata_expanded[Assay_Type] = []
        for row in options.metadata_expanded[Assay_Type]:
            fhw.write(",".join([row[0], #organism
                              row[1], #tissue
                              row[2], #Layout
                              row[3], #Assay_type
                              row[4],
                              row[5],
                              row[6]  
                ])+"\n")
            
            fhw.write(",".join([row[0],
                              row[1],
                              "SE",
                              row[3],
                              row[4],
                              row[5],
                              row[6]  
                ])+"\n")
            add_these_back_to_metadata_expanded[Assay_Type].append([row[0],row[1],"SE",row[3],row[4],row[5],row[6]])
    fhw.close()
    for Assay_Type in options.metadata_expanded:
        options.metadata_expanded[Assay_Type].extend(add_these_back_to_metadata_expanded[Assay_Type])
    logging.info(f"Metadata preparation for alignment is complete")
    
    ##################################################################################################
    # Map reads with STAR
    ##################################################################################################
    cmd  = "align_reads_using_STAR.py "
    cmd += f" --metadata {options.output_directory}/metadata_for_alignment.csv "
    cmd += f" --output_directory {options.output_directory}/star_alignments "
    cmd += f" --star_genome_index {options.output_directory}/star_index "
    cmd += f" --input_location {options.output_directory}/raw_data "
    cmd += f" --cpu {options.cpu} "
    cmd += f" --num_times {options.num_times} "
    cmd += f" 1> {options.output_directory}/outputs/star_alignment.output "
    cmd += f" 2> {options.output_directory}/errors/star_alignment.error "
    os.system(cmd)
    logging.info(f"Aligning reads with STAR is complete")
    
    ##################################################################################################
    # Compress alignments with Abridge
    ##################################################################################################
    for iteration in range(int(options.num_times)):
        for Assay_Type in options.metadata_expanded:
            for row in options.metadata_expanded[Assay_Type]:
                sra = row[6]
                layout = row[2]
                alignment_samfilename = f"{options.output_directory}/star_alignments/{sra}_{layout}.sam"
            
                cmd  = f"(/usr/bin/time --verbose abridge "
                cmd += f" --compress "
                cmd += f" --cpu 1 "
                cmd += f" --genome {options.genome} "
                cmd += f" --inputsamfilenames {alignment_samfilename} "
                cmd += f" --output_directory {options.output_directory}/abridge_compressed_files/{iteration} "
                cmd += f" --error_directory {options.error_directory}/abridge/{iteration} "
                cmd += f" --keep_intermediate_error_files "
                cmd += f" --both " # Perform both compression - once using 7z and again using brotli (from Google)
                cmd += ")"
                cmd += f" 1> {options.output_directory}/outputs/{sra}_{iteration}_{layout}_compress.output "
                cmd += f" 2> {options.output_directory}/errors/{sra}_{iteration}_{layout}_compress.error " 
                if os.path.exists(f"{options.output_directory}/abridge_compressed_files/{iteration}/{sra}_{layout}.sam.abridge.7z") == True and os.path.exists(f"{options.output_directory}/abridge_compressed_files/{iteration}/{sra}_{layout}.sam.abridge.br") == True : continue
                logging.info(f"Running command - {cmd}")
                os.system(cmd)
                break
            break
        break
                
    ##################################################################################################
    # Decompress alignments with Abridge
    ##################################################################################################
    for iteration in range(int(options.num_times)):
        for Assay_Type in options.metadata_expanded:
            for row in options.metadata_expanded[Assay_Type]:
                sra = row[6]
                layout = row[2]
                alignment_samfilename = f"{options.output_directory}/star_alignments/{sra}_{layout}.sam"
            
                cmd  = f"(/usr/bin/time --verbose abridge "
                cmd += f" --decompress "
                cmd += f" --cpu 1 "
                cmd += f" --genome {options.genome} "
                cmd += f" --inputabrfilenames {options.output_directory}/abridge_compressed_files/{iteration}/{sra}_{layout}.sam.abridge.br "
                cmd += f" --output_directory {options.output_directory}/abridge_decompressed_files/{iteration} "
                cmd += f" --error_directory {options.error_directory}/abridge/{iteration} "
                cmd += f" --keep_intermediate_error_files "
                cmd += ")"
                cmd += f" 1> {options.output_directory}/outputs/{sra}_{iteration}_{layout}_decompress.output "
                cmd += f" 2> {options.output_directory}/errors/{sra}_{iteration}_{layout}_decompress.error " 
                if os.path.exists(f"{options.output_directory}/abridge_compressed_files/{iteration}/{sra}_{layout}.decompress.sam") == True : continue
                logging.info(f"Running command - {cmd}")
                os.system(cmd)
                break
            break
        break
    

if __name__ == "__main__":
    main()
    
    
    
    
    